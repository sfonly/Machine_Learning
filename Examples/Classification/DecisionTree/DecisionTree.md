# 决策树
## 1 决策树原理
**决策树定义：**  
决策树是一种树形结构。其中，每个内部节点表示一个属性上的特征情况，每个分支代表该节点特征的不同输出，而每个叶子节点代表一种类别。决策树算法就是要利用已有信息，来构造一个决策树，对于测试信息的分类，将按照决策上的各个分支来进行，满足对应条件，就将进入到该分支，直到叶子节点。

**如何选取分支条件，构造一颗高效的决策树呢?**   
一般算法中都是以信息熵为判定标准。信息论之父香农借鉴了热力学的概念，把信息中排除了冗余后的平均信息量称为“信息熵”，并给出了计算信息熵的数学表达式。  

    信息熵公式

信息熵即是表示事件所包含信息量大小的概念。我们所知事件的信息量越小，事件的发生概率越不确定，信息熵越大；我们所知事件的信息量越多，事件发生的概率越确定，信息熵越小，比如：
1.	太阳从东方升起  
这是一个确定事件，一定会发生，概率为1，通过这句话，我们几乎不能获得新的信息、新的知识，因此，我们认为信息量很小，信息熵为0。  
2.	中国足球队将参加2022年世界杯并取得冠军  
几乎是一个确定事件，一定不会发生，概率约等于0，通过这句话，我们几乎不能获得新的信息、新的知识，因此，我们认为信息量很小，信息熵为0。  
3.	一群好朋友，天气好，就去打篮球，天气差，就不去。这个情况下，我们知道了明天的天气是否要下雨。  
原来的事件是一个混沌的情况，即不知道天气的情况下，我不知道这个出去打篮球这个事件会不会发生，概率为50%，即一半一半，信息熵为1。当我们知道了明天要下雨，那么打篮球这个事件就不会发生了，概率为0%，信息熵也变为了0；天气好，不下雨的情况同理。通过获得明天天气的这个信息，将原来不确定的系统，变为了一个确定的系统，降低了混乱程度，使得信息的熵值变为了0，获得的信息增益为1，即这个事件的带来的信息量极大，信息增益极大。

        Gain = Entroy（旧系统）- Entroy（新系统）=1

**总结：**  
在决策树算法中，找出对预测事件信息增益大的属性来构造分支，通过这样的方式就能选择合适的建立分枝的属性，从而构造一个高效的决策树。  
决策树中最常用的三种基础算法：  
（1）	ID3: 基于信息增益  
（2）	C4.5: 基于信息增益率  
（3）	CART: 基于基尼系数

**优缺点：**  
三种决策树中，ID3和C4.5是用信息增益进行计算，并且都是采用局部最优化算法来选择分枝属性。  
  * ID3不能处理连续数据，需要把连续数据进行分箱离散化后才能处理，ID3会在前几层分枝过程中，会优先选择分枝数多的，能分得更完全的特征来建树，这会导致树的宽度极大，在实际的树模型实现中，检索效率较低，且分类效果不佳。
  * C4.5在ID3的基础上做了一定的改进，可以处理连续数据，并且采用了信息增益率来解决选择分枝特征值过多的问题，并且在缺失值和异常值上变现较好
  * CART是分类回归树，可用于分类问题和回归问题，也是随机森林中最常用的树型结构。CART一定是一个二叉树，可以自动寻找连续数据最优切割点，并且可以重复利用某个特征（切分得更细）。

需要注意的是：采用单一的决策树模型，谨慎的进行剪枝来处理过拟合问题。

## 2 案例背景
鸢尾花数据集可能是模式识别、机器学习等领域里被使用最多的一个数据集了，很多教材用这份数据来做案例，很多工具，包括R、scikit-learn，都会自带这些数据集，而且说学术界很多论文也应用这份数据做实验。
鸢尾花数据集最初由Edgar Anderson 测量得到，而后在著名的统计学家和生物学家R.A Fisher于1936年发表的文章「The use of multiple measurements in taxonomic problems」中被使用，用其作为线性判别分析（Linear Discriminant Analysis）的一个例子，证明分类的统计方法，从此而被众人所知，尤其是在机器学习这个领域。
数据中的两类鸢尾花记录结果是在加拿大加斯帕半岛上，于同一天的同一个时间段，使用相同的测量仪器，在相同的牧场上由同一个人测量出来的。这是一份有着70年历史的数据，虽然老，但是却很经典。




